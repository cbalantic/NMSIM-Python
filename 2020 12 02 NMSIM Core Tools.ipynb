{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model DENA Flight Tracks with NMSIM\n",
    "\n",
    "---\n",
    "\n",
    "## Import libraries and <font color=\"orange\">set paths</font>\n",
    "<font color=\"orange\">lines 30 - 63 may need editing for your computer's specific configuration *(but hopefully not!)*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ Import Libraries =======================\n",
    "\n",
    "# some very standard libraries\n",
    "import sys\n",
    "import datetime as dt\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import shutil\n",
    "import subprocess\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:.5f}'.format\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from itertools import islice\n",
    "\n",
    "# geoprocessing libraries\n",
    "import fiona\n",
    "from fiona.crs import from_epsg\n",
    "import pyproj\n",
    "import geopandas as gpd\n",
    "from shapely.ops import transform\n",
    "from shapely.geometry import mapping, Point, Polygon\n",
    "\n",
    "# We also need two specialized NPS libaries: `soundDB` and `iyore`\n",
    "# we expect them in the same directory as this repository\n",
    "try:\n",
    "    sys.path.append(os.path.join(os.path.dirname(os.getcwd()), \"soundDB\"))\n",
    "    from soundDB import *\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Can't find library `soundDB`, please clone the repository from https://github.com/gjoseph92/soundDB to\",\n",
    "          os.path.dirname(os.getcwd()))\n",
    "\n",
    "try:\n",
    "    sys.path.append(os.path.join(os.path.dirname(os.getcwd()), \"iyore\"))\n",
    "    import iyore\n",
    "    \n",
    "except ModuleNotFoundError:\n",
    "    print(\"Can't find library `iyore`, please clone the repository from https://github.com/nationalparkservice/iyore to\",\n",
    "          os.path.dirname(os.getcwd()), \"or install using pip\")\n",
    "\n",
    "\n",
    "# ============ Set up a few paths and connections ===============\n",
    "\n",
    "# DENA RDS computer\n",
    "RDS = r\"\\\\inpdenards\\overflights\"\n",
    "\n",
    "try:\n",
    "    sys.path.append(os.path.join(RDS, \"scripts\"))\n",
    "    from query_tracks import query_tracks\n",
    "    \n",
    "except:\n",
    "    print(\"While importing `query_tracks` encountered an error.\")\n",
    "\n",
    "# DENA Render computer (and an iyore dataset)\n",
    "Render = r\"\\\\inpdenarender\\E\\Sound Data\"\n",
    "archive = iyore.Dataset(Render)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: from `tracks_within`\n",
    "connection_txt=r\"L:\\config\\connection_info.txt\" # line 162; Sam's database connection information\n",
    "\n",
    "# :: where are the overflights functions?\n",
    "r\"L:\\scripts\"\n",
    "\n",
    "# load the `soundDB` function\n",
    "r\"C:\\Users\\DBetchkal\\PythonScripts\\3 GITHUB REPOSITORIES\\soundDB\"\n",
    "\n",
    "# where are the acoustic metadata?\n",
    "r\"V:\\Complete_Metadata_AKR_2001-2020.txt\"\n",
    "\n",
    "# where are the acoustic data\n",
    "archive = iyore.Dataset(r\"E:\") # define a dataset object\n",
    "\n",
    "# A CENTRALIZED PROJECT DIRECTORY\n",
    "project_dir = r\"C:\\Users\\DBetchkal\\Desktop\\NMSIM_2014_local\\Data\\DENASAN4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(368652.0635772837, 7031955.267847246)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unit = \"DENA\"\n",
    "site = \"BULL\"\n",
    "year = 2019 \n",
    "\n",
    "\n",
    "# load the metadata sheet\n",
    "metadata = pd.read_csv(r\"\\\\inpdenafiles\\sound\\Complete_Metadata_AKR_2001-2020.txt\", \n",
    "                       delimiter=\"\\t\", encoding = \"ISO-8859-1\")\n",
    "\n",
    "# look up the site's coordinates in WGS84\n",
    "lat_in, long_in = metadata.loc[(metadata[\"code\"] == site)&(metadata[\"year\"] == year), \"lat\":\"long\"].values[0]\n",
    "\n",
    "# lookup the UTM zone using the first point\n",
    "zone = get_utm_zone(long_in)\n",
    "print(zone)\n",
    "\n",
    "# epsg codes for Alaskan UTM zones\n",
    "epsg_lookup = {1:'epsg:26901', 2:'epsg:26902', 3:'epsg:26903', 4:'epsg:26904', 5:'epsg:26905', \n",
    "               6:'epsg:26906', 7:'epsg:26907', 8:'epsg:26908', 9:'epsg:26909', 10:'epsg:26910'}\n",
    "\n",
    "# convert from D.d (WGS84) to meters (NAD83)\n",
    "projector = pyproj.Transformer.from_crs('epsg:4326', epsg_lookup[zone])\n",
    "\n",
    "# convert into NMSIM's coordinate system\n",
    "lat, long = projector.transform(lat_in, long_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "- Define a study area.\n",
    "- Prepare NMSIM baselayers and canonical data structure\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===========================  Define functions  =======================================\n",
    "\n",
    "def get_utm_zone(longitude):\n",
    "    \n",
    "    return (int(1+(longitude+180.0)/6.0))\n",
    "\n",
    "\n",
    "def climb_angle(v):\n",
    "    \n",
    "    \"\"\"\n",
    "    compute the 'climb angle' of a vector\n",
    "    A = ùëõ‚Ä¢ùëè=|ùëõ||ùëè|ùë†ùëñùëõ(ùúÉ)\n",
    "    \"\"\"\n",
    "    \n",
    "    # a unit normal vector perpendicular to the xy plane\n",
    "    n = np.array([0,0,1])\n",
    "    \n",
    "    degrees = np.degrees(np.arcsin( np.dot(n, v)/(np.linalg.norm(n)*np.linalg.norm(v))))\n",
    "    return degrees\n",
    "\n",
    "\n",
    "def point_buffer(lat, lon, km):\n",
    "    \n",
    "    wgs84 = pyproj.Proj(init='epsg:4326')\n",
    "\n",
    "    # Azimuthal equidistant projection\n",
    "    aeqd_formatter = '+proj=aeqd +lat_0={lat} +lon_0={lon} +x_0=0 +y_0=0'\n",
    "    aeqd = pyproj.Proj(aeqd_formatter.format(lat=lat, lon=lon))\n",
    "\n",
    "    # project the site coordinate into aeqd\n",
    "    long_m, lat_m = pyproj.transform(wgs84, aeqd, lon, lat)\n",
    "\n",
    "    # buffer using a radius in meters\n",
    "    buf_m = Point(long_m, lat_m).buffer(km * 1000)  # distance in metres\n",
    "\n",
    "    # this will convert polygons from aeqd back into wgs84\n",
    "    projector = partial(pyproj.transform, aeqd, wgs84)\n",
    "\n",
    "    buf = transform(projector, buf_m)  # apply projection\n",
    "\n",
    "    return buf\n",
    "\n",
    "\n",
    "def tracks_within(ds, site, year, search_within_km = 20, climb_ang_max = 20, aircraft_specs=False, clip=False, altOut=None):\n",
    "    \n",
    "    unit = \"DENA\"\n",
    "    \n",
    "    # ===== first part; site coordinate wrangling =====================\n",
    "    \n",
    "    # load the metadata sheet\n",
    "    metadata = pd.read_csv(r\"\\\\inpdenafiles\\sound\\Complete_Metadata_AKR_2001-2020.txt\", \n",
    "                           delimiter=\"\\t\", encoding = \"ISO-8859-1\")\n",
    "\n",
    "    # look up the site's coordinates in WGS84\n",
    "    lat_in, long_in = metadata.loc[(metadata[\"code\"] == site)&(metadata[\"year\"] == year), \"lat\":\"long\"].values[0]\n",
    "\n",
    "    # lookup the UTM zone using the first point\n",
    "    zone = get_utm_zone(long_in)\n",
    "\n",
    "    # epsg codes for Alaskan UTM zones\n",
    "    epsg_lookup = {1:'epsg:26901', 2:'epsg:26902', 3:'epsg:26903', 4:'epsg:26904', 5:'epsg:26905', \n",
    "                   6:'epsg:26906', 7:'epsg:26907', 8:'epsg:26908', 9:'epsg:26909', 10:'epsg:26910'}\n",
    "\n",
    "    # convert from D.d (WGS84) to meters (NAD83)\n",
    "    in_proj = pyproj.Proj(init='epsg:4326')\n",
    "    out_proj = pyproj.Proj(init=epsg_lookup[zone])\n",
    "\n",
    "    # convert into NMSIM's coordinate system\n",
    "    long, lat = pyproj.transform(in_proj, out_proj, long_in, lat_in)\n",
    "\n",
    "\n",
    "    # ===== second part; mic height to feet =====================\n",
    "\n",
    "    # look up the site's coordinates in WGS84\n",
    "    height = metadata.loc[(metadata[\"code\"] == site)&(metadata[\"year\"] == year), \"microphone_height\"].values[0]\n",
    "\n",
    "    print(unit+site+str(year)+\":\", \"{0:.0f},\".format(long), \"{0:.0f}\".format(lat), \"- UTM zone\", zone)\n",
    "    print(\"\\tmicrophone height\", \"{0:.2f} feet.\".format(height*3.28084))\n",
    "\n",
    "\n",
    "    # ===== third part; save mask file using the buffer radius of choice ===============\n",
    "\n",
    "    # create the buffer polygon\n",
    "    buf = point_buffer(lat_in, long_in, search_within_km)  \n",
    "\n",
    "    # Define a polygon feature geometry with one attribute\n",
    "    schema = {'geometry': 'Polygon', 'properties': {'id': 'int'},}\n",
    "\n",
    "    # write a new shapefile with the buffered polygon\n",
    "    with fiona.open('site_buf.shp', 'w', 'ESRI Shapefile', schema, crs=from_epsg(4326)) as c:\n",
    "        \n",
    "        ## If there are multiple geometries, put the \"for\" loop here\n",
    "        c.write({'geometry': mapping(buf),'properties': {'id': 0},})\n",
    "\n",
    "    print(\"\\n\\tShapefile containing \" + str(search_within_km)+\"km radius buffer has been written!\")\n",
    "    \n",
    "    # plot the buffer within the park boundary\n",
    "    buffer_path = os.path.join(os.getcwd(), \"site_buf.shp\")\n",
    "    DENA_outline_path = r\"T:\\ResMgmt\\WAGS\\Sound\\GIS\\Denali\\DENA_outline.shp\"\n",
    "    gpd_buffer = gpd.read_file(buffer_path)\n",
    "    gpd_DENA = gpd.read_file(DENA_outline_path)\n",
    "    base = gpd_buffer.plot(color='white', edgecolor='black')\n",
    "    base.set_aspect(2)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    # ===== fourth part; determine the date range of NVSPL files ===============\n",
    "\n",
    "    # load the datetime of every NVSPL file\n",
    "    NVSPL_dts = pd.Series([dt.datetime(year=int(e.year),\n",
    "                                       month=int(e.month),\n",
    "                                       day=int(e.day),\n",
    "                                       hour=int(e.hour),\n",
    "                                       minute=0,\n",
    "                                       second=0) for e in ds.nvspl(unit=unit, site=site, year=year)])\n",
    "\n",
    "    # everything should be in chronological order, but just in case...\n",
    "    NVSPL_dts.sort_values(inplace=True, ascending=True)\n",
    "\n",
    "    # retrieve the start/end bounds and convert back to YYYY-MM-DD strings\n",
    "    start, end = (dt.datetime.strftime(d, \"%Y-%m-%d\") for d in [NVSPL_dts.iloc[0], NVSPL_dts.iloc[-1]])\n",
    "    print(\"\\n\\tRecord begins\", start, \"and ends\", end, \"\\n\")\n",
    "    \n",
    "    # load tracks from the database over a certain daterange, using the buffered site\n",
    "    tracks = query_tracks(connection_txt=os.path.join(RDS, \"config\\connection_info.txt\"), \n",
    "                          start_date=start, end_date=end, \n",
    "                          mask=gpd_buffer, clip_output=clip,\n",
    "                          aircraft_info=aircraft_specs)\n",
    "    \n",
    "    # make a dataframe to hold distances and times\n",
    "    closest_approaches = pd.DataFrame([], index=np.unique(tracks[\"id\"]), columns=[\"closest_distance\", \"closest_time\"])\n",
    "    \n",
    "    # process each unique flight track in sequence\n",
    "    for f_id, data in tracks.groupby(\"flight_id\"):\n",
    "        \n",
    "        if(len(data) > 1): # \"one point does not a trajectory make\"\n",
    "        \n",
    "            # double check that the data are sorted by time\n",
    "            data = data.sort_values(\"ak_datetime\")\n",
    "\n",
    "            long_utm, lat_utm = pyproj.transform(in_proj, out_proj, data[\"longitude\"].values, data[\"latitude\"].values)\n",
    "\n",
    "            # assign back into the dataframe for subsequent use\n",
    "            data.loc[:, \"long_UTM\"] = long_utm\n",
    "            data.loc[:, \"lat_UTM\"] = lat_utm\n",
    "\n",
    "            # coordinates of each point\n",
    "            coords = np.array([[lo, la, e] for lo, la, e in zip(data[\"long_UTM\"], \n",
    "                                                                data[\"lat_UTM\"], \n",
    "                                                                0.3048*data[\"altitude_ft\"])])\n",
    "\n",
    "            # convert the coordinates to vectors\n",
    "            V = np.diff(coords, axis=0)\n",
    "\n",
    "            # compute the climb angle for each point: \n",
    "            climb_angs = np.array([climb_angle(V[n]) for n in np.arange(len(V))])\n",
    "\n",
    "            # if the climb angle is greater than 20¬∞ something strange is going on: reset to zero\n",
    "            climb_angs[np.abs(climb_angs) > climb_ang_max] = 0\n",
    "\n",
    "            # we don't know what the last point's angle should be, but NMSIM requires a value\n",
    "            # just repeat the penultimate point's value\n",
    "            climb_angs = np.append(climb_angs, climb_angs[-1])\n",
    "\n",
    "            # assign the climb angles to the array\n",
    "            data[\"ClimbAngle\"] = climb_angs\n",
    "\n",
    "            # NMSIM probably won't like nan... replace those with zero as well\n",
    "            data[\"ClimbAngle\"].fillna(0, inplace=True)\n",
    "\n",
    "            # this is the start time of the track\n",
    "            start = data[\"ak_datetime\"].iloc[0]\n",
    "\n",
    "            # truncate starting time to nearest hour to check against the NVSPL list\n",
    "            check = start.replace(minute=0, second=0)\n",
    "\n",
    "            # 1 if there is a match, 0 if not\n",
    "            match_bool = NVSPL_dts.apply(lambda date: date in [check]).sum()\n",
    "\n",
    "            if(match_bool == 0):\n",
    "\n",
    "                tracks = tracks[tracks[\"flight_id\"] != f_id]\n",
    "                print(\"\\t\\t\", \"Flight starting\", start, \"has no matching acoustic record\")\n",
    "\n",
    "            elif(match_bool == 1):\n",
    "                \n",
    "                if(altOut is not None):\n",
    "\n",
    "                    # make a folder for NMSIM .trj outputs\n",
    "                    trj_out = altOut\n",
    "                    if not os.path.exists(trj_out):\n",
    "                        os.makedirs(trj_out)\n",
    "                        \n",
    "                else:\n",
    "                    \n",
    "                    # a path to the site's computational outputs folder\n",
    "                    cOut = [e.path for e in ds.dataDir(unit=unit, site=site, year=year)][0] + \\\n",
    "                            os.sep + \"02 ANALYSIS\\Computational Outputs\"\n",
    "                    \n",
    "                    # make a folder for NMSIM .trj outputs\n",
    "                    trj_out = cOut + os.sep + \"NMSIM_trj\"\n",
    "                    if not os.path.exists(trj_out):\n",
    "                        os.makedirs(trj_out)\n",
    "\n",
    "                # find the time at which the flight passes closest to the station\n",
    "                site_coords = np.array([long, lat])\n",
    "                GPSpoints_xy = np.array([data[\"long_UTM\"], data[\"lat_UTM\"]])\n",
    "\n",
    "                # which point made the closest approach to the site?\n",
    "                min_distance = np.min(np.linalg.norm(site_coords - GPSpoints_xy.T, axis=1))/1000\n",
    "\n",
    "                # keep track of the closest approach by id\n",
    "                closest_approaches.loc[data[\"flight_id\"].iloc[0], \"closest_distance\"] = min_distance\n",
    "\n",
    "                # at what time did the closest approach occur?\n",
    "                closest_time = data.iloc[np.argmin(np.linalg.norm(site_coords - GPSpoints_xy.T, axis=1))]['ak_datetime']\n",
    "\n",
    "                # likewise keep track of the closest time\n",
    "                closest_approaches.loc[data[\"flight_id\"].iloc[0], \"closest_time\"] = closest_time\n",
    "\n",
    "                print(\"\\t\\t\", \"#\"+str(f_id), \"expected closest at\", closest_time, \"{0:0.1f}km\".format(min_distance))\n",
    "\n",
    "                # create a time-elapsed column\n",
    "                data[\"time_elapsed\"] = (data[\"ak_datetime\"] - data[\"ak_datetime\"].min()).apply(lambda t: t.total_seconds())\n",
    "\n",
    "                # we'll only save the trajectory if it's within the specified search radius!\n",
    "                if(min_distance <= search_within_km):\n",
    "\n",
    "                    # ======= densify the GPS points for NMSIM ========\n",
    "\n",
    "                    print(\"\\n\\t\\t\\t\", \"Flight is within search distance. Attempting to densify\", \n",
    "                          data.shape[0], \"points...\")\n",
    "\n",
    "                    new_points = gpd.GeoDataFrame([])\n",
    "                    for row in data.itertuples():\n",
    "\n",
    "                        try:\n",
    "\n",
    "                            next_ind = data.index[np.argwhere(data.index == row.Index)+1][0][0]\n",
    "\n",
    "                            # this represents time steps < 1 second\n",
    "                            interpSteps = int(1.1*(data.loc[next_ind, \"ak_datetime\"] - row.ak_datetime).total_seconds())\n",
    "                            print(\"trying for\", interpSteps, \"steps between points\")\n",
    "\n",
    "                            # interpolate the indices, longitudes, latitudes, and altitudes\n",
    "                            # we don't need the first or last values - they're already in the dataframe\n",
    "                            indi = np.linspace(row.Index, next_ind, interpSteps)[1:-1]\n",
    "                            ti = np.linspace(row.time_elapsed, data.loc[next_ind, \"time_elapsed\"], interpSteps)[1:-1]\n",
    "                            xi = np.linspace(row.longitude, data.loc[next_ind, \"longitude\"], interpSteps)[1:-1]\n",
    "                            yi = np.linspace(row.latitude, data.loc[next_ind, \"latitude\"], interpSteps)[1:-1]\n",
    "                            zi = np.linspace(row.altitude_ft, data.loc[next_ind, \"altitude_ft\"], interpSteps)[1:-1]\n",
    "                            utm_xi = np.linspace(row.long_UTM, data.loc[next_ind, \"long_UTM\"], interpSteps)[1:-1]\n",
    "                            utm_yi = np.linspace(row.lat_UTM, data.loc[next_ind, \"lat_UTM\"], interpSteps)[1:-1]\n",
    "                            cai = np.linspace(row.ClimbAngle, data.loc[next_ind, \"ClimbAngle\"], interpSteps)[1:-1]\n",
    "                            hi = np.linspace(row.heading, data.loc[next_ind, \"heading\"], interpSteps)[1:-1]\n",
    "                            vi = np.linspace(row.knots, data.loc[next_ind, \"knots\"], interpSteps)[1:-1]\n",
    "\n",
    "                            # generate geometry objects for each new interpolated point\n",
    "                            gi = [Point(xyz) for xyz in zip(xi, yi, zi)]\n",
    "\n",
    "                            # create a dictionary of the interpolated values to their column\n",
    "                            d = {'time_elapsed': ti,\n",
    "                                 'longitude': xi, \n",
    "                                 'latitude': yi,\n",
    "                                 'altitude_ft': zi,\n",
    "                                 'long_UTM': utm_xi,\n",
    "                                 'lat_UTM': utm_yi,\n",
    "                                 'ClimbAngle': cai,\n",
    "                                 'heading': hi,\n",
    "                                 'knots': vi,\n",
    "                                 'geom': gi}\n",
    "\n",
    "\n",
    "                            # turn the newly interpolated values into a GeoDataFrame \n",
    "                            rowsi = gpd.GeoDataFrame(d, index=indi, crs=\"EPSG:4326\")\n",
    "\n",
    "                            # append to the track's overall new points\n",
    "                            new_points = new_points.append(rowsi)\n",
    "\n",
    "                        # there is no next index on the last row... pass through\n",
    "                        except IndexError:\n",
    "                            pass\n",
    "\n",
    "\n",
    "                    # append the new points and sort by index (which is also by time)\n",
    "                    data = data.append(new_points)\n",
    "                    data = data.sort_index()\n",
    "\n",
    "                    print(\"\\t\\t\\t\", \"...trajectory now has\", \n",
    "                          data.shape[0], \"points!\\n\")\n",
    "\n",
    "                    # ======= write the trajectory file! ==============\n",
    "\n",
    "                    print(\"\\t\\t\\t\", \"Densification complete, writing trajectory file...\")\n",
    "\n",
    "                    # add N-number and begin time\n",
    "                    start_time = dt.datetime.strftime(data[\"utc_datetime\"].min(), \"%Y-%m-%d %H:%M:%S\")\n",
    "                    file_name_dt = dt.datetime.strftime(data[\"utc_datetime\"].min(), \"_%Y%m%d_%H%M%S\")\n",
    "                    N_number = data[\"registration\"].iloc[0]\n",
    "\n",
    "                    # path to the specific .trj file to be written\n",
    "                    trj_path = trj_out + os.sep + str(N_number) + str(file_name_dt) + \".trj\"\n",
    "\n",
    "                    with open(trj_path, 'w') as trajectory:\n",
    "\n",
    "                        # write the header information\n",
    "                        trajectory.write(\"Flight track trajectory variable description:\\n\")\n",
    "                        trajectory.write(\" time - time in seconds from the reference time\\n\")\n",
    "                        trajectory.write(\" Xpos - x coordinate (UTM)\\n\")\n",
    "                        trajectory.write(\" Ypos - y coordinate (UTM)\\n\")\n",
    "                        trajectory.write(\" UTM Zone  \"+str(zone)+\"\\n\")\n",
    "                        trajectory.write(\" Zpos - z coordinate in meters MSL\\n\")\n",
    "                        trajectory.write(\" heading - aircraft compass bearing in degrees\\n\")\n",
    "                        trajectory.write(\" climbANG - aircraft climb angle in degrees\\n\")\n",
    "                        trajectory.write(\" vel - aircraft velocity in knots\\n\")\n",
    "                        trajectory.write(\" power - % engine power\\n\")\n",
    "                        trajectory.write(\" roll - bank angle (right wing down), degrees\\n\")\n",
    "                        trajectory.write(\"FLIGHT \" + str(N_number) + \" beginning \" + start_time +\" UTC\\n\")\n",
    "                        trajectory.write(\"TEMP.  59.0\\n\")\n",
    "                        trajectory.write(\"Humid.  70.0\\n\")\n",
    "                        trajectory.write(\"\\n\")\n",
    "                        trajectory.write(\"         time(s)        Xpos           Ypos           Zpos         heading        climbANG       Vel            power          rol\\n\")\n",
    "\n",
    "                        # now write the data section row by row\n",
    "                        for ind, point in data.iterrows():\n",
    "\n",
    "                            # write the line\n",
    "                            trajectory.write(\"{0:15.3f}\".format(point[\"time_elapsed\"]) + \\\n",
    "                                             \"{0:15.3f}\".format(point[\"long_UTM\"]) + \\\n",
    "                                             \"{0:15.3f}\".format(point[\"lat_UTM\"]) + \\\n",
    "                                             \"{0:15.3f}\".format(0.3048*point[\"altitude_ft\"]) + \\\n",
    "                                             \"{0:15.3f}\".format(point[\"heading\"]) + \\\n",
    "                                             \"{0:15.3f}\".format(point[\"ClimbAngle\"]) + \\\n",
    "                                             \"{0:15.3f}\".format(point[\"knots\"]) + \\\n",
    "                                             \"{0:15.3f}\".format(95) + \\\n",
    "                                             \"{0:15.3f}\".format(0) + \"\\n\")\n",
    "\n",
    "                        print(\"\\t\\t\\t...finished writing .trj\", \"\\n\")\n",
    "                        print(\"-----------------------------------------------------------------------------------------\")\n",
    "\n",
    "                \n",
    "                # (closes `if` from line 253) the flight was not within the search radius...\n",
    "                else:\n",
    "                    tracks = tracks[tracks[\"flight_id\"] != f_id] # ...drop the flight ID from the table\n",
    "                    print(\"\\t\\t\", \"Flight starting\", start, \"was not within the search radius.\")\n",
    "    \n",
    "    print(tracks.size)\n",
    "    if(tracks.shape[0] <= 1):\n",
    "        \n",
    "        print(\"\\nSorry, no tracks in the database coincide with this deployment.\")\n",
    "        \n",
    "        tracks = gpd.GeoDataFrame([], columns=tracks.columns) # return an empty geodataframe with the original columns\n",
    "        return tracks\n",
    "    \n",
    "    elif(tracks.shape[0] > 1):\n",
    "        \n",
    "        u = np.unique(tracks[\"id\"])\n",
    "        print(\"\\nThere are\", len(u), \"tracks in the database which coincide with this deployment.\")\n",
    "        print(\"Identification numbers:\", u)\n",
    "        \n",
    "        # iterate through each id number and add the closest approach information\n",
    "        for track_id, flight in closest_approaches.iterrows():\n",
    "            \n",
    "            tracks.loc[tracks[\"flight_id\"] == track_id, \"closest_time\"] = flight[\"closest_time\"]\n",
    "            tracks.loc[tracks[\"flight_id\"] == track_id, \"closest_distance\"] = flight[\"closest_distance\"]\n",
    "        \n",
    "        return tracks\n",
    "    \n",
    "    \n",
    "def NMSIM_create_tis(project_dir, source_path, Nnumber=None, NMSIMpath=None):\n",
    "    \n",
    "    '''\n",
    "    Create a site-based model run (.tis) using the NMSIM batch processor.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    None\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # ======= (1) define obvious, one-to-one project files ================\n",
    "    \n",
    "    elev_file = project_dir + os.sep + r\"Input_Data\\01_ELEVATION\" + os.sep + \"elevation.flt\"\n",
    "    \n",
    "    # imped_file = project_dir + os.sep + \"Input_Data\\01_IMPEDANCE\" + os.sep + \"landcover.flt\"\n",
    "    imped_file = None\n",
    "\n",
    "    trj_files = glob.glob(project_dir + os.sep + r\"Input_Data\\03_TRAJECTORY\\*.trj\")\n",
    "\n",
    "    # eventually the batch file is going to want this\n",
    "    tis_out_dir = project_dir + os.sep + r\"Output_Data\\TIG_TIS\"\n",
    "    \n",
    "    # ======= (2) define less obvious project files - these still need thought! ================\n",
    "    \n",
    "    # site files need some thinking through... there COULD be more than one per study area\n",
    "    # (it's quite project dependant)\n",
    "    site_file = glob.glob(project_dir + os.sep + r\"Input_Data\\05_SITES\\*.sit\")[0]\n",
    "    \n",
    "    # strip out the FAA registration number\n",
    "    registrations = [t.split(\"_\")[-3][11:] for t in trj_files]\n",
    "\n",
    "    # the .tis name preserves: reciever + source + time (roughly 'source : path : reciever')\n",
    "    site_prefix = os.path.basename(site_file)[:-4]\n",
    "\n",
    "    tis_files = [tis_out_dir + os.sep + site_prefix + \"_\" + os.path.basename(t)[:-4] for t in trj_files]\n",
    "\n",
    "    trajectories = pd.DataFrame([registrations, trj_files, tis_files], index=[\"N_Number\",\"TRJ_Path\",\"TIS_Path\"]).T\n",
    "    \n",
    "    # ======= (3) write the control + batch files for command line control of NMSIM ================\n",
    "        \n",
    "    # set up the two files we want to write\n",
    "    control_file = project_dir + os.sep + \"control.nms\"\n",
    "    batch_file = project_dir + os.sep + \"batch.txt\"\n",
    "    \n",
    "    # select the trajectories to process\n",
    "    if(Nnumber == None):\n",
    "\n",
    "        trj_to_process = trajectories\n",
    "    \n",
    "    else:\n",
    "\n",
    "        trj_to_process = trajectories.loc[trajectories[\"N_Number\"] == Nnumber, :]\n",
    "    \n",
    "    \n",
    "    if(NMSIMpath == None):\n",
    "        \n",
    "        # assume that the project folder is in \"..\\NMSIM_2014\\Data\"\n",
    "        # and look for Nord2000batch.exe two directories up\n",
    "        Nord = os.path.dirname(os.path.dirname(project_dir)) + os.sep + \"Nord2000batch.exe\"\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        Nord = NMSIMpath\n",
    "\n",
    "    for meta, flight in trj_to_process.iterrows():\n",
    "    \n",
    "\n",
    "        # write the control file for this situation\n",
    "        with open(control_file, 'w') as nms:\n",
    "\n",
    "            nms.write(elev_file+\"\\n\") # elevation path\n",
    "            \n",
    "            if(imped_file != None):\n",
    "                nms.write(imped_file+\"\\n\") # impedance path\n",
    "            else:\n",
    "                nms.write(\"-\\n\")\n",
    "                \n",
    "            nms.write(site_file+\"\\n\") # site path\n",
    "            nms.write(flight[\"TRJ_Path\"]+\"\\n\")\n",
    "            nms.write(\"-\\n\")\n",
    "            nms.write(\"-\\n\")\n",
    "            nms.write(source_path+\"\\n\")\n",
    "            nms.write(\"{0:11.4f}   \\n\".format(500.0000))\n",
    "            nms.write(\"-\\n\")\n",
    "            nms.write(\"-\")    \n",
    "\n",
    "        # write the batch file to create a site-based analysis\n",
    "        with open(batch_file, 'w') as batch:\n",
    "\n",
    "            batch.write(\"open\\n\")\n",
    "            batch.write(control_file+\"\\n\")\n",
    "            batch.write(\"site\\n\")\n",
    "            batch.write(flight[\"TIS_Path\"]+\"\\n\")\n",
    "            batch.write(\"dbf: no\\n\")\n",
    "            batch.write(\"hrs: 0\\n\")\n",
    "            batch.write(\"min: 0\\n\")\n",
    "            batch.write(\"sec: 0.0\")\n",
    "        \n",
    "        # ======= (4) compute the theoretically observed trace on the site's microphone ================\n",
    "        \n",
    "        print(flight[\"TRJ_Path\"]+\"\\n\")\n",
    "        \n",
    "        process = subprocess.Popen([Nord, batch_file], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "        stdout, stderr = process.communicate()\n",
    "\n",
    "        output_messages = stdout.decode(\"utf-8\").split(\"\\r\\n\")\n",
    "        output_messages = [ out for out in output_messages if out.strip() != '' ]\n",
    "        \n",
    "        print(\"\\tthe following lines are directly from NMSIM:\")\n",
    "        for s in output_messages+[\"\\n\"]:\n",
    "            print(\"\\t\"+s)\n",
    "\n",
    "        # slightly messier printing for error messages\n",
    "        if(stderr != None):\n",
    "            for s in sterr.decode(\"utf-8\").split(\"\\r\\n\"):\n",
    "                print(s.strip()) \n",
    "                \n",
    "\n",
    "def pair_trj_to_tis_results(project_dir):\n",
    "    \n",
    "    '''\n",
    "    Join a directory of .tis results created by NMSIM\n",
    "      to the .trj files that created them.\n",
    "      \n",
    "    Inputs\n",
    "    ------\n",
    "    project_dir (str): the path to a canonical NPS-style NMSIM project directory\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    iterator (zip object): an iterator containing the paired .tis and .trj file paths\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # find all the '.tis' files\n",
    "    successful_tis = glob.glob(project_dir + os.sep + \"Output_Data\\TIG_TIS\\*.tis\")\n",
    "\n",
    "    # find all the '.trj' files\n",
    "    trajectories = [project_dir + os.sep + \"Input_Data\\\\03_TRAJECTORY\" + \\\n",
    "                    os.sep + os.path.basename(f)[9:-4] + \".trj\" for f in successful_tis]\n",
    "    \n",
    "    iterator = zip(trajectories, successful_tis)\n",
    "    \n",
    "    return iterator\n",
    "\n",
    "\n",
    "def tis_resampler(tis_path, dt_start, utc_offset=-8):\n",
    "    \n",
    "    '''\n",
    "    '''\n",
    "    \n",
    "    # read the data line-by-line\n",
    "    with open(tis_path) as f:\n",
    "\n",
    "        content = list(islice(f, 18 + (3600*24)))\n",
    "    \n",
    "    \n",
    "    # find the line index where the header ends\n",
    "    splitBegin = content.index('---End File Header---\\n')\n",
    "\n",
    "    # take out the whitespace and two empty columns at either end\n",
    "    spectral_data = [re.split(r'\\s+',c) for c in content[splitBegin+10:]] \n",
    "    spectral_data = [d[1:-2] for d in spectral_data]      \n",
    "    \n",
    "    # initalize a pandas dataframe using the raw spectral data and the expected column headers\n",
    "    tis = pd.DataFrame(spectral_data, columns=[\"SP#\",\"TIME\",\"F\",\"A\",\"10\", \"12.5\",\"15.8\",\"20\",\"25\",\"31.5\",\"40\",\"50\",\"63\",\n",
    "                                                \"80\",\"100\",\"125\",\"160\",\"200\",\"250\",\"315\",\"400\",\"500\",\"630\",\"800\",\"1000\",\n",
    "                                                \"1250\",\"1600\",\"2000\",\"2500\",\"3150\",\"4000\",\"5000\",\"6300\",\"8000\",\"10000\",\n",
    "                                                \"12500\"], dtype='float') #,\"20000\"\n",
    "\n",
    "    # there's a weird text line at the end of the file (is this true for all .tis files?)\n",
    "    tis.drop(tis.tail(1).index,inplace=True) # drop last n rows\n",
    "\n",
    "    # these columns are stubborn\n",
    "    tis[\"TIME\"] = tis[\"TIME\"].astype('float')\n",
    "    tis[\"SP#\"] = tis[\"SP#\"].astype('float').apply(lambda f: int(f))\n",
    "    tis[\"F\"] = tis[\"F\"].astype('int')\n",
    "\n",
    "    # convert relevant columns to decibels (dB) from centibels (cB)\n",
    "    tis.loc[:,'A':'12500'] *= 0.1\n",
    "\n",
    "    # timedelta to adjust to local time\n",
    "    utc_offset = dt.timedelta(hours=utc_offset) \n",
    "\n",
    "    # reindex the dataframe to AKT\n",
    "    tis.index = tis[\"TIME\"].astype('float').apply(lambda t: dt_start + dt.timedelta(seconds=t) + utc_offset)\n",
    "\n",
    "    # resample to match NVSPL time resolution\n",
    "    clean_tis = tis.sort_index().resample('1S').quantile(0.5)\n",
    "    \n",
    "    return clean_tis\n",
    "\n",
    "\n",
    "def NVSPL_to_match_tis(ds, clean_tis, trj, unit, site, year, utc_offset=-8, pad_length=5):\n",
    "    \n",
    "    '''\n",
    "    '''\n",
    "    \n",
    "    # timedelta to adjust to local time\n",
    "    utc_offset = dt.timedelta(hours=utc_offset) \n",
    "    \n",
    "    # convert startdate to Alaska Time\n",
    "    ak_start = startdate + utc_offset\n",
    "    \n",
    "    # tidy up the TIS spectrogram by converting np.nan to -99.9\n",
    "    clean_tis.fillna(-99.9).values.T\n",
    "    \n",
    "    # we can only compare 1/3rd octave bands down to 12.5 Hz... drop the rest\n",
    "    clean_tis = clean_tis.loc[:, ~clean_tis.columns.isin([\"SP#\", \"TIME\", \"F\", \"A\", \"10\"])]\n",
    "\n",
    "    # load NVSPL for the day of the event\n",
    "    nv = nvspl(ds,\n",
    "               unit=unit,\n",
    "               site=site,\n",
    "               year=ak_start.year,\n",
    "               month=str(ak_start.month).zfill(2),\n",
    "               day=str(ak_start.day).zfill(2),\n",
    "               hour=[str(h).zfill(2) for h in np.unique(clean_tis.index.hour.values)],\n",
    "               columns=[\"H\"+s.replace(\".\", \"p\") for s in clean_tis.columns]).combine()\n",
    "\n",
    "    # if multiple hours, drop the heirarchical index\n",
    "    if isinstance(nv.index, pd.MultiIndex):\n",
    "        nv.index = nv.index.droplevel(0)\n",
    "\n",
    "    # select the SPL data that corresponds \n",
    "    pad = dt.timedelta(minutes=pad_length)\n",
    "    \n",
    "    # find the NVSPL data the specifically corresponds to the timing of the model\n",
    "    event_SPL = nv.loc[clean_tis.index[0]-pad : clean_tis.index[-1]+pad,:]\n",
    "    \n",
    "    print(\"NVSPL shape:\", event_SPL.shape)\n",
    "\n",
    "    # pad the theoretical data as well\n",
    "    spect_pad = np.full((int(pad.total_seconds()), clean_tis.shape[1]), -99.9)\n",
    "    theoretical = np.vstack((spect_pad, clean_tis))\n",
    "\n",
    "    print(\"NMSIM shape:\", theoretical.shape)\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(18,5), sharex=True)\n",
    "    \n",
    "    # convert the NVSPL's nice datetime axis to numbers\n",
    "    x_lims = mdates.date2num(event_SPL.index)\n",
    "    \n",
    "    ax[0].set_title(\"NMISIM results\", loc=\"left\")\n",
    "    ax[0].imshow(theoretical.T, aspect='auto', origin='lower', \n",
    "                 extent=[x_lims[0], x_lims[-1], 0, event_SPL.shape[1]],\n",
    "                 cmap='plasma', interpolation=None, vmin=-10, vmax=80, zorder=-5)\n",
    "\n",
    "    ax[0].set_yticks(np.arange(event_SPL.shape[1])[::4])\n",
    "    ax[0].set_yticklabels(event_SPL.columns.astype('float')[::4])\n",
    "    \n",
    "    # tell matplotlib that the numeric axis should be formatted as dates\n",
    "    ax[0].xaxis_date()\n",
    "    ax[0].xaxis.set_major_formatter(mdates.DateFormatter(\"%b-%d\\n%H:%M\")) # tidy them!\n",
    "    \n",
    "    ax[1].set_title(\"microphone measurement at \"+unit+site, loc=\"left\")\n",
    "    im = ax[1].imshow(event_SPL.T, aspect='auto', origin='lower', \n",
    "                      extent=[x_lims[0], x_lims[-1], 0, event_SPL.shape[1]],\n",
    "                      cmap='plasma', interpolation=None, vmin=-10, vmax=80)\n",
    "    \n",
    "    # the same as for the first plot\n",
    "    ax[1].set_yticks(np.arange(event_SPL.shape[1])[::4])\n",
    "    ax[1].set_yticklabels(event_SPL.columns.astype('float')[::4])\n",
    "    ax[1].xaxis_date()\n",
    "    ax[1].xaxis.set_major_formatter(mdates.DateFormatter(\"%b-%d\\n%H:%M\")) # tidy them!\n",
    "\n",
    "    fig.colorbar(im, ax=ax.ravel().tolist(), anchor=(2.2, 0.0))\n",
    "    fig.text(1.06, 0.5, \"Sound Level (Leq, 1s)\", va='center', rotation='vertical', fontsize=10)\n",
    "    fig.text(-0.02, 0.55, \"Frequency Band (Hz)\", va='center', rotation='vertical', fontsize=13)\n",
    "    \n",
    "    title = os.path.basename(trj)[:-4]\n",
    "    \n",
    "    plt.suptitle(title, y=1.05, fontsize=17, ha=\"center\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.savefig(project_dir + os.sep + r\"Output_Data\\IMAGES\" + os.sep + title + \"_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    return event_SPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Reference:</font> using `iyore` what sites correspond to the GPS data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'iyore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d5ffd7d794fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0marchive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miyore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"E:\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# define a dataset object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# find Denali sites from 2019, 2020\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msite\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marchive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataDir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"DENA\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myear\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"2019\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"2020\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'iyore' is not defined"
     ]
    }
   ],
   "source": [
    "archive = iyore.Dataset(r\"E:\") # define a dataset object\n",
    "\n",
    "# find Denali sites from 2019, 2020\n",
    "np.unique([e.site for e in archive.dataDir(unit=\"DENA\", year=[\"2019\", \"2020\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert GPS points to NMSIM *.trj*\n",
    "Given a NPS monitoring site... and how far away are you interested in vehicle motion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# archive = iyore.Dataset(r\"E:\\Sound Data\") # \\\\INPDENARENDER\n",
    "archive = iyore.Dataset(r\"E:\") # \\\\INPDENARENDER\n",
    "\n",
    "# ============= EDIT THESE ===================================================\n",
    "\n",
    "# project directory for the current site of interest\n",
    "# project_dir = r\"C:\\Users\\ahug\\Documents\\NMSim_2014\\Data\\DENAPRM4\"\n",
    "project_dir = r\"C:\\Users\\DBetchkal\\Desktop\\NMSIM_2014_local\\Data\\DENASAN4\"\n",
    "\n",
    "year = 2019\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# extract the site name from the project directory path\n",
    "site = project_dir[-4:]\n",
    "\n",
    "tracks = tracks_within(archive, site, year, search_within_km = 20, aircraft_specs=False,\n",
    "                      altOut = project_dir + os.sep + r\"Input_Data\\03_TRAJECTORY\")\n",
    "\n",
    "\n",
    "# which N-Numbers were observed on this record?\n",
    "tracks.registration.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a control file 'from scratch'\n",
    "\n",
    ">*elevation_path* <br>\n",
    ">*impedance_path* (or empty) <br>\n",
    ">*site_path* <br>\n",
    ">*trajectory_path* <br>\n",
    ">*-* <br>\n",
    ">*-* <br>\n",
    ">*source_path* <br>\n",
    ">*contour interval (m) as* `\"{0:11.4f}   \".format(500.0000)` <br>\n",
    ">*-* <br>\n",
    ">*-* <br>\n",
    "\n",
    "### Write a batch file 'from scratch'\n",
    "\n",
    ">**open** <br>\n",
    ">*control_file_path* <br>\n",
    ">**site** <br>\n",
    ">*tis_file_output* (no extension) <br>\n",
    ">**dbf: no** <br>\n",
    ">**hrs: 0** <br>\n",
    ">**min: 0** <br>\n",
    ">**sec: 0.0** <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the models in NMSIM\n",
    "#### Repeat for each N-Number, *then* proceed to the next cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= EDIT THESE ===================================================\n",
    "\n",
    "# project directory for the current site of interest\n",
    "# project_dir = r\"C:\\Users\\ahug\\Documents\\NMSim_2014\\Data\\DENAPRM4\"\n",
    "project_dir = r\"C:\\Users\\DBetchkal\\Desktop\\NMSIM_2014_local\\Data\\DENASAN4\"\n",
    "\n",
    "# FAA registry for aircraft of interest\n",
    "Focal_NNumber = \"N21HY\"\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# extract the site name from the project directory path\n",
    "site = project_dir[-4:]\n",
    "\n",
    "# NMSIM program directiory\n",
    "NMSIMpath = os.path.dirname(os.path.dirname(project_dir))\n",
    "\n",
    "# lookup for appropriate NMSIM source file\n",
    "source_map = {\"N8888\": NMSIMpath + os.sep + \"Sources\\MiscellaneousSources\\omni.src\",\n",
    "              \"N709M\": NMSIMpath + os.sep + \"Sources\\AirTourFixedWingSources\\C182.src\",\n",
    "              \"N570AE\": NMSIMpath + os.sep + \"Sources\\AirTourHelicopterSources\\AS350.src\",\n",
    "               \"N74PS\": NMSIMpath + os.sep + \"Sources\\AirTourFixedWingSources\\C207.src\",\n",
    "              \"N619CH\": NMSIMpath + os.sep +  \"Sources\\AirTourFixedWingSources\\C207.src\",\n",
    "              \"N72309\": NMSIMpath + os.sep + \"Sources\\AirTourFixedWingSources\\C207.src\",\n",
    "              \"N72395\": NMSIMpath + os.sep + \"Sources\\AirTourFixedWingSources\\C207.src\",\n",
    "              \"N473YC\": NMSIMpath + os.sep + \"Sources\\AirTourFixedWingSources\\C207.src\",\n",
    "              \"N21HY\":  NMSIMpath + os.sep + \"Sources\\AirTourFixedWingSources\\C182.src\"}\n",
    "\n",
    "\n",
    "# run the NMSIM model\n",
    "NMSIM_create_tis(project_dir, source_map[Focal_NNumber], Nnumber=Focal_NNumber)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NMSIM results and pair with NPS acoustic measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iyore dataset for access to the measurements\n",
    "archive = iyore.Dataset(r\"E:\") # define a dataset object)\n",
    "\n",
    "# NMSIM isn't always successful - we want to iterate only through files that WERE created\n",
    "trj_and_tis = pair_trj_to_tis_results(project_dir)\n",
    "\n",
    "diagnostic = True\n",
    "\n",
    "for trj, tis in trj_and_tis:\n",
    "    \n",
    "    runName = os.path.basename(trj)[:-4]\n",
    "    \n",
    "    # read just the header of the trajectory\n",
    "    with open(trj) as lines:\n",
    "        head = [next(lines) for x in range(12)]\n",
    "\n",
    "    # here's the string containing the starting time (to the second!)\n",
    "    dateString = head[-1][-24:-5]\n",
    "\n",
    "    # convert to a datetime object\n",
    "    startdate = dt.datetime.strptime(dateString, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    print(\"now working on run:\", runName)\n",
    "    \n",
    "    try:\n",
    "        # this is the theoretical 1/3rd octave band trace\n",
    "        theory = tis_resampler(tis, startdate)\n",
    "        \n",
    "    except:\n",
    "        print(\"tis\", tis)\n",
    "        print(\"produced a ValueError related to nan values in the .tis file\")\n",
    "    \n",
    "    \n",
    "    # compare and save results\n",
    "    event_SPL = NVSPL_to_match_tis(archive, theory, trj, \n",
    "                                   unit=\"DENA\", site=site, year=int(startdate.year),\n",
    "                                   utc_offset=-8, pad_length=5)\n",
    "    \n",
    "    # show diagnostic plots\n",
    "    if(diagnostic == True):\n",
    "        \n",
    "        plt.figure(figsize=(14, 3))\n",
    "        plt.plot(theory.index, theory[\"A\"], zorder=5, ls=\"\", marker=\"o\", ms=2)\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
